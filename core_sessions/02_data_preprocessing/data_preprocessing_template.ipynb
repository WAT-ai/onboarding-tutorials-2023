{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Preprocessing\n",
    "Prerequisites:\n",
    "- Python experience, including a basing understanding of python syntax, loops, conditional statements, functions, and data types in python\n",
    "- Some background in statistics is helpful\n",
    "\n",
    "Goals for this session:\n",
    "- Learn to perform basic exploratory data analysis (EDA) and data visualization\n",
    "- Identify outliers, handle missing values, and perform other common data operations such as normalization, interpolation, and filtering\n",
    "- Understand the intuition behind various preprocessing techniques for both categorical and continuous features\n",
    "- Apply EDA and data preprocessing techniques to a novel data set without context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Date fruit and Pumpkin seed dataset\n",
    "\n",
    "# Techniques\n",
    "In this code-along exercise, we will cover the following data processing techniques:\n",
    "- Basic data views\n",
    "- Renaming columns with `df.rename()`\n",
    "- Filtering, viewing subsets with `df.loc[]`\n",
    "- Sorting with `df.sort()`\n",
    "- Merges with `pd.join` or `df.merge`\n",
    "- Removing outliers with custom functions\n",
    "- Normalization\n",
    "\n",
    "[Pandas documentation](https://pandas.pydata.org/docs/reference/frame.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports\n",
    "By convention, we use the `as` in the import statement to alias `numpy` to `np`. Similarly, we alias `pandas` to `pd`. Another convention we will use is calling Pandas DataFrame objects `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note that the Jupyter Notebook also prints out the number of rows and columns of the DataFrame when we just let it autoprint the DataFrame. We can calso view the shape of the DataFrame using `df.shape`. By convention, the first number in the tuple is the number of rows and the second number is the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can access the index and column axes of the DataFrame with `df.index` and `df.columns` respectively. The values of the DataFrame can be access with `df.values` which returns a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Other conventions\n",
    "- Specify one column with `\"column name\"`, specify multiple columns with `[\"list\", \"of\", \"column\", \"names\"]`\n",
    "- Index (`axis=0`) contains a unique identifier for each of the rows; Columns (`axis=1`) contain a unique identifier for each of the columns\n",
    "- Most operations default to applying to the Index axis. It's best practice to specify the axis directly for clairty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renaming Columns\n",
    "Analyzing the column names, we notice a few things about the dataset. \n",
    "\n",
    "Of benefit:\n",
    "1. Both datasets use many of the same features\n",
    "2. Both datasets use the underscores to delimit words in the column names\n",
    "\n",
    "Of concern:\n",
    "1. Both datasets use different capitalization \n",
    "2. The feature of \"equivalent diameter\" in the pumpkin seeds to be \"equivalent diameter squared\" in the date fruits\n",
    "3. The major and minor axis features are missing the word \"length\" for the data fruits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumpkin_seed_features = [\n",
    "    'area',\n",
    "    'perimeter',\n",
    "    'major_axis_length',\n",
    "    'minor_axis_length',\n",
    "    'convex_area',\n",
    "    'equiv_diameter',\n",
    "    'eccentricity',\n",
    "    'solidity',\n",
    "    'extent',\n",
    "    'roundness',\n",
    "    'aspect_ratio',\n",
    "    'compactness',\n",
    "    'class'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this section, we will deal with these concerns by renaming the columns using `df.rename()`.\n",
    "\n",
    "`df.rename()` has two forms:\n",
    "- `df.rename(mapper=function, axis=\"columns\")` where we specify a function that is applied to all the column names. The function should result in all unique outputs when applied to all column names.\n",
    "- `df.rename(mapper=Dict, axis=\"columns\")` where we would specify a dictionary with keys as the old column names and values as the new column names. The columns to rename do not have to exist in the DataFrame; any keys in the dict that are not present will have no effect on the DataFrame and no error will be raised.\n",
    "\n",
    "To deal with the capitalization, we will apply a function that transforms the string into all lowercase. To deal with the different names, we will make a dictionary that maps the lower-case incorrect names to correct ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make mapper dict\n",
    "\n",
    "# Make mapper function\n",
    "\n",
    "# Apply to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing subsets\n",
    "Just selecting data from a dataframe will not apply the operation to the dataframe, unless we assign it to a variable.\n",
    "\n",
    "To view just a single columns, we can index the dataframe with that column name. Note that a single column will be returned as a `Series` object, which is similar to a dataframe, but with just one column. Therefore, some operations cannot be applied as with dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the area column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view multiple columns, pass in a list of column names. When multiple columns are selected, a `DataFrame` is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the area and perimieter columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to select rows. If you know the row index or indicies, you can use `df.iloc[i]` or `df.iloc[[i, j, k]]` to select those rows.\n",
    "\n",
    "If you know a condition on the values, you should use `df.loc[index_mask, columns]`. `index_mask` Boolean mask is a `(n,)` sized array of `True` or `False` values, while `columns` is a list of the columns. If `columns` are not specified, it will be assumed that all columns are selected, while rows will be filtered according to `index_mask`.\n",
    "\n",
    "The mask can be made from taking a Series and applying a Boolean logic operator to it. For example, \n",
    "`mask = df[col] == value`. Multiple masks can be combined with AND (`&`) and OR (`|`) elementwise operators, with mask expressions in brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows 1, 3, and 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the area and perimeter of BERHI date fruits whose area is greater than 500_000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort a dataframe by the values in a specific column using `df.sort_values(by=column_name)`. By default, sorting is by ascending order, so specifying `ascending=False` will sort in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort large BERHI dates by perimeter in descending order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the column names are normalized and we know how to select specific columns, we can select the same subset of columns that the pumpkin seeds dataset has.\n",
    "\n",
    "We could drop the unwanted columns one-by-one using `df.drop(labels=[\"column\", \"names\"], axis=1)`. Some operations can also be applied to the same object by specifing the keyword `inplace=True`. By default, `inplace=False`, so changes will not be applied. Here, we will drop the unwanted columns `[\"meanrr\", \"meanrg\", \"meanrb\"]` in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since we know the columns we want to keep, we could just select them and assign the result to `df` to keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier removal\n",
    "\n",
    "Outliers are data points that differ significantly from the mean distribution of the data. Often, outliers can represent incorrect measurements, although domain knowledge is often required to interpret the meaning of outliers. We will only analyze the feature columns (e.g. not `class`) for outliers, since they're all continuous values, whereas `class` is categorical.\n",
    "\n",
    "In this example, we can view the distributions of each of the features and note that there is an outlier with the aspect ratio with an abnormally large value, where we would typically expect this feature to be between 1-3. There are some compactness and roundness values near 0, where we expect these to be near 1. The solidity feature also has a long left tail, so some lower values might also be considered outliers.\n",
    "\n",
    "There are many ways to remove outliers, like using heuristic rules. There are also statistical methods. In this example, we'll compute a robust Z-score using $z' = \\frac{x - \\tilde{x}}{1.4826 \\times \\text{MAD}(x)}$ where $\\tilde{x}$ is the median of $x$, MAD is the median absolute deviation, and 1.4826 is a scale factor so that the MAD approximates the standard deviation of a normal distribution. \n",
    "\n",
    "We will reject outliers as having a robust Z-score of $|z'| > 6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution\n",
    "fig, axs = plt.subplots(3, 4, figsize=(8, 6))\n",
    "\n",
    "feature_cols = df.columns.drop(\"class\")\n",
    "\n",
    "for col, ax in zip(feature_cols, axs.flatten()):\n",
    "    ax.hist(df[col].values, bins=20)\n",
    "    ax.set_xlabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute some stats\n",
    "from scipy.stats import median_abs_deviation\n",
    "\n",
    "df_score = df.copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply robust Z-score to each column\n",
    "for col in feature_cols:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask is a (n, d) array of (0, 1) values\n",
    "mask = (df_score[feature_cols] > 6) | (df_score[feature_cols] < -6)\n",
    "mask.sum(axis=\"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summing mask along columns reveals which rows are outliers\n",
    "df.loc[mask.sum(axis=\"columns\") != 0].sort_values(by=\"area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers by only selecting non-outlier indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution after outlier removal\n",
    "fig, axs = plt.subplots(3, 4, figsize=(8, 6))\n",
    "\n",
    "for col, ax in zip(feature_cols, axs.flatten()):\n",
    "    ax.hist(df[col].values, bins=20)\n",
    "    ax.set_xlabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature normalization\n",
    "\n",
    "As discussed earlier, machine learning models require their inputs to be in vectors. Additionally, a lot of ML models assume that the input data is roughly normally distributed with 0 mean and unit standard deviation, or that the data is scaled between [-1, 1]. By preparing data to have a standard normal distribution generally improves model performance compared to having un-normalized features.\n",
    "\n",
    "Scikit-learn has a handy interface to apply common normalization techniques like standard scaling and min-max scaling to dataframes.\n",
    "\n",
    "We only apply this normalization to the feature columns, e.g. not the `class` column. Later, we will talk about how to encode categorical variables like `class` for ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select feautre columns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standard scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean and standard deviation of the features are now 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_trans.agg([\"mean\", \"std\"]).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can retreive the feature matrix and pass this to a machine learning model as the input features! Note, we would still need to encode the labels (`df[\"class\"]`) as a vector first before being able to do supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive feature matrix as df values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Working with Different Data Types\n",
    "\n",
    "The candy dataset contains survey results. Respondents were asked to rate various Halloween candies as 'Meh', 'Joy', or 'Despair' based on how happy they would be to receive a particular candy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candy = pd.read_excel('../data/candy.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Ordinal Categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The candy dataset contains mostly categorical data. Many machine learning models require vector inputs with numerical data. So, a very common operation is to encode categorical variables, effectively converting them into integer format.\n",
    "\n",
    "Ordinal categoricals are discrete categories that have some order. Think \"small\", \"medium\", \"large\". One technique for converting nominal categorical variables to numeric values is label encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "One particular type of candy can have 3 ratings. These ratings have an implicit order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A natural solution would be to assign DESPAIR a value of -1, MEH a value of 0, and JOY a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Nominal Categoricals\n",
    "\n",
    "Nominal categoricals have no order, so concepts such as mean, min, and max have no interpretation. One particularly popular method of encoding, known as One-Hot Encoding involves representing categorical variables as binary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Converting to Categorical\n",
    "\n",
    "Suppose you are interested in comparing candy ratings across age groups. Looking at ratings for each individual age does not make practical sense, so you could bin the age column into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "candy['Q3: AGE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "age_groups = candy[candy['Q3: AGE'].apply(lambda x: str(x).isnumeric())]['Q3: AGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "age_groups.apply(lambda x: round(x, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Handling Missing Values\n",
    "\n",
    "Not all missing values are equal. Consider arbitrary survey data taken from the general population.\n",
    "\n",
    "**Missing Not At Random** - when a value is missing for a reason related to the true value. (Ex: if a survey responding chooses not to disclose their income, this could be because they have an abnormally high or low income)\n",
    "\n",
    "**Missing at Random** - when a value is missing for a reason related to another observed variable. (Ex: many age values are missing for survey respondents of a particular gender)\n",
    "\n",
    "**Missing Completely at Random** - when there's no patterns in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solar_df = pd.read_csv(\"../data/solarenergy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "solar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Deletion\n",
    "\n",
    "- Column deletion: removing a column that has too many missing values and is non-essential for your model\n",
    "- Row deletion: removing rows with missing values, ideally if the missing values are Missing At Random, to avoid biasing your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "solar_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "solar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Two ways of making row deletion persistent in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# solar_df.dropna(inplace=True)\n",
    "# solar_df = solar_df.dropna(subset=['solar radiation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "solar_df.drop('solar radiation')\n",
    "#solar_ df.drop('solar radiation', axis=1) # set axis=1 to drop a row!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imputation\n",
    "\n",
    "- Fill missing values with their defaults (empty string, zero, etc...)\n",
    "- Fill missing values with the mean, median, or mode\n",
    "- Backward or forward fill\n",
    "- Imputation risks injecting your own bias and adding noise to the data, and should be performed with caution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# don't worry too much about this line\n",
    "null_indices = solar_df.loc[pd.isna(solar_df['solar radiation']), :].index\n",
    "interp = interp_df.iloc[null_indices]\n",
    "\n",
    "plt.plot(solar_df['time'].values[:100], solar_df['solar radiation'].values[:100])\n",
    "plt.scatter(interp.time.values[:10], interp['solar radiation'].values[:10], color='orange')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
