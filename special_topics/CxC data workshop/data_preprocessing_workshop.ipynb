{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Preprocessing\n",
    "Prerequisites:\n",
    "- Python experience, including a basing understanding of python syntax, loops, conditional statements, functions, and data types in python\n",
    "- Some background in statistics is helpful\n",
    "\n",
    "Goals for this session:\n",
    "- Learn to perform basic exploratory data analysis (EDA) and data visualization\n",
    "- Identify outliers, handle missing values, and perform other common data operations such as normalization and covariate analysis\n",
    "- Understand the intuition behind various preprocessing techniques for both categorical and continuous features\n",
    "- Apply EDA and data preprocessing techniques to the Titanic dataset from Kaggle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Titanic Dataset\n",
    "In this code-along exercise, we will cover the following data processing techniques:\n",
    "- Basic DataFrame operations\n",
    "- Handling missing values\n",
    "- Engineering new features\n",
    "- Removing outliers\n",
    "- Normalization\n",
    "- Converting categorical variables to numbers\n",
    "\n",
    "[Pandas documentation](https://pandas.pydata.org/docs/reference/frame.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports\n",
    "By convention, we use the `as` in the import statement to alias `numpy` to `np`. Similarly, we alias `pandas` to `pd`. Another convention we will use is calling Pandas DataFrame objects `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load the data\n",
    "The [Titanic dataset](https://www.kaggle.com/competitions/titanic/data) contains information regarding the survival of Titanic passengers.\n",
    "\n",
    "In this section, we are going to examine the data and handle cases with outliers and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use this in Colab\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/WAT-ai/onboarding-tutorials-2023/main/special_topics/CxC%20data%20workshop/data/titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# DataFrame basics\n",
    "\n",
    "To view the first `n` lines of a DataFrame, we can use `df.head(n)`. By default, `n = 5`. Similarly, to view the last `n` lines, we can use `df.tail(n)`.\n",
    "\n",
    "In the Jupyter Notebook, the last line of each cell executed prints to the notebook. Simply writing `df` as the last line in a cell will print out an abridged version of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note that the Jupyter Notebook also prints out the number of rows and columns of the DataFrame when we just let it autoprint the DataFrame. We can can so view the shape of the DataFrame using `df.shape`. By convention, the first number in the tuple is the number of rows and the second number is the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can access the index and column axes of the DataFrame with `df.index` and `df.columns` respectively. The values of the DataFrame can be access with `df.values` which returns a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Other conventions\n",
    "- Specify one column with `\"column name\"`, specify multiple columns with `[\"list\", \"of\", \"column\", \"names\"]`\n",
    "- Index (`axis=0`) contains a unique identifier for each of the rows; Columns (`axis=1`) contain a unique identifier for each of the columns\n",
    "- Most operations default to applying to the Index axis. It's best practice to specify the axis directly for clairty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Viewing subsets\n",
    "Just selecting data from a dataframe will not apply the operation to the dataframe, unless we assign it to a variable.\n",
    "\n",
    "To view just a single columns, we can index the dataframe with that column name. Note that a single column will be returned as a `Series` object, which is similar to a dataframe, but with just one column. Therefore, some operations cannot be applied as with dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select the name column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To view multiple columns, pass in a list of column names. When multiple columns are selected, a `DataFrame` is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View the Name and Sex columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are two ways to select rows. If you know the row index or indicies, you can use `df.iloc[i]` or `df.iloc[[i, j, k]]` to select those rows.\n",
    "\n",
    "If you know a condition on the values, you should use `df.loc[index_mask, columns]`. `index_mask` Boolean mask is a `(n,)` sized array of `True` or `False` values, while `columns` is a list of the columns. If `columns` are not specified, it will be assumed that all columns are selected, while rows will be filtered according to `index_mask`.\n",
    "\n",
    "The mask can be made from taking a Series and applying a Boolean logic operator to it. For example,\n",
    "`mask = df[col] == value`. Multiple masks can be combined with AND (`&`) and OR (`|`) elementwise operators, with mask expressions in brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select rows 1, 3, and 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View the name and class of men whose age is greater than 70\n",
    "mask = (df[\"Sex\"] == \"male\") & (df[\"Age\"] > 70)\n",
    "df.loc[mask, [\"Name\", \"Pclass\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Distributions of continuous variables against a categorical variable\n",
    "sns.kdeplot(data=df, x='Age', hue='Sex', multiple=\"stack\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplots of continuous variables\n",
    "g = sns.pairplot(df[[\"Age\", \"Fare\", \"Parch\", \"SibSp\"]], kind=\"kde\")\n",
    "g.map_offdiag(sns.kdeplot, fill=True, color='C0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data\n",
    "On a small dataset like there, here's an easy way to visualize how much data is missing. Clearly, many passengers are missing a Cabin, and some are missing an age. Oddly, at least one passenger has no information on where they embarked from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isna(), cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select where Embarked is NA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to do with missing values is a design decision.**\n",
    "\n",
    "One option would be to drop the entire Embarked column. It might appear as if the port where passengers embark impacts their likelihood of survival, but higher class passengers were also more likely to board at certain ports. This feature might not actually be useful in classifying survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df.Embarked, y=df.Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df.Embarked, y=df.Pclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On the other hand, you could simply drop the two rows with missing values, or try to impute those values using the most likely port that they boarded at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values\n",
    "\n",
    "Not all missing values are equal. Consider arbitrary survey data taken from the general population.\n",
    "\n",
    "**Missing Not At Random** - when a value is missing for a reason related to the true value. (Ex: if a survey responding chooses not to disclose their income, this could be because they have an abnormally high or low income)\n",
    "\n",
    "**Missing at Random** - when a value is missing for a reason related to another observed variable. (Ex: many age values are missing for survey respondents of a particular gender)\n",
    "\n",
    "**Missing Completely at Random** - when there's no patterns in the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deletion\n",
    "\n",
    "- Column deletion: removing a column that has too many missing values and is non-essential for your model\n",
    "- Row deletion: removing rows with missing values, ideally if the missing values are Missing At Random, to avoid biasing your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Ticket', 'Cabin', 'Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=\"Embarked\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deletions can persist if we assign the result to a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=\"Embarked\", axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "- Fill missing values with their defaults (empty string, zero, etc...)\n",
    "- Fill missing values with the mean, median, or mode\n",
    "- Backward or forward fill\n",
    "- Imputation risks injecting your own bias and adding noise to the data, and should be performed with caution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].fillna(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].median(), df['Age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will fill missing ages with the mean age. \n",
    "\n",
    "Another way to make operations persist is to use `inplace=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "1. The 'Name' category doesn't tell us much on its own, but the titles can be useful as identifiers. We will extract the titles from the names, and map these to categories, creating a new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'] = df['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_mapping = {\n",
    "    'Mme': 'Mrs',\n",
    "    'Ms': 'Mrs',\n",
    "    'Mlle': 'Miss',\n",
    "    'Dr': 'Officer',\n",
    "    'Rev': 'Officer',\n",
    "    'Col': 'Officer',\n",
    "    'Major': 'Officer',\n",
    "    'Capt': 'Officer',\n",
    "    'Don': 'Noble',\n",
    "    'Sir': 'Noble',\n",
    "    'Lady': 'Noble',\n",
    "    'the Countess': 'Noble',\n",
    "    'Jonkheer': 'Noble'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'] = df['Title'].map(title_mapping).fillna(df['Title']) # any title not in the mapping is left unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The 'SibSp' feature tells us about how many siblings or spouses a passenger has onboard. The 'Parch' feature tells us about the number of parents and children a passenger has onboard. Together, we can create a 'Family Size' feature that represents known relatives. Note that in a linear regression model, linear combinations of features are redundant, but still shown here as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Family_Size'] = df['SibSp'] + df['Parch'] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Only first class has Cabin numbers, but they are listed in formats like \"G6\" or \"D10\". Let's convert these to a 'Deck' category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to help us parse the cabin numbers\n",
    "def substrings_in_string(big_string, substrings):\n",
    "    for substring in substrings:\n",
    "        if big_string.find(substring) != -1:\n",
    "            return substring\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'Unknown']\n",
    "df['Deck'] = df['Cabin'].map(lambda x: substrings_in_string(str(x), cabin_list)).fillna('No Cabin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People who had cabins were more likely to survive than those who didn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df.Deck, y=df.Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df.Deck, y=df.Fare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making new features, we should drop the columns that were used to make those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Ticket', 'Cabin', 'Name', 'SibSp', 'Parch'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Outlier removal\n",
    "\n",
    "Outliers are data points that differ significantly from the mean distribution of the data. Often, outliers can represent incorrect measurements, although domain knowledge is often required to interpret the meaning of outliers. We will only analyze the numerical features that are not innately categorical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the datatypes and our understanding of the values in the columns, we should only analyze SibSp, Parch, Fare, and Age for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting the distribution\n",
    "fig, axs = plt.subplots(1, 3, figsize=(6, 3))\n",
    "\n",
    "feature_cols = [\"Age\", \"Family_Size\", \"Fare\"]\n",
    "\n",
    "for col, ax in zip(feature_cols, axs.flatten()):\n",
    "    ax.hist(df[col].values, bins=20)\n",
    "    ax.set_xlabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these disributions, there aren't any unreasonable values. But for the sake of this workshop, we will remove outliers from the `Fare` column. We will use the heuristic that any values that are 6 standard deviations from the mean, i.e. $x \\notin [\\mu - 6\\sigma, \\mu + 6\\sigma]$, are outliers. Note that this heuristic typically assumes that the distribution is normal, however, this is clearly not the case for Fare. \n",
    "\n",
    "Other methods to remove outliers include 1.5*IQR, using median and median absolute deviation, and percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quick stats on numeric columns with df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = None\n",
    "ub = None\n",
    "lb, ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out how many outliers there are\n",
    "outlier_mask = (df[\"Fare\"] < lb) | (df[\"Fare\"] > ub)\n",
    "outlier_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers by selecting non-outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Feature normalization\n",
    "\n",
    "As discussed earlier, machine learning models require their inputs to be in vectors. Additionally, a lot of ML models assume that the input data is roughly normally distributed with 0 mean and unit standard deviation, or that the data is scaled between [-1, 1]. By preparing data to have a standard normal distribution generally improves model performance compared to having un-normalized features. Note that many features will not actually have Gaussian or Uniform distributions, but we still apply this scaling anyways.\n",
    "\n",
    "Scikit-learn has a handy interface to apply common normalization techniques like standard scaling and min-max scaling to entire dataframes.\n",
    "\n",
    "Here, we will do do standard scaling with just pandas, since many columns here are not relevant to scaling. We only apply this normalization to the numeric feature columns, e.g. Age, Fare, Family_Size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalization manually\n",
    "for feature in [\"Age\", \"Fare\", \"Family_Size\"]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Age\", \"Fare\", \"Family_Size\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see that the mean and standard deviation of the features are now 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Ordinal Categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The Titanic Dataset contains plenty of categorical data. Many machine learning models require vector inputs with numerical data. So, a very common operation is to encode categorical variables, effectively converting them into integer format.\n",
    "\n",
    "Ordinal categoricals are discrete categories that have some order. Think \"small\", \"medium\", \"large\". One technique for converting nominal categorical variables to numeric values is label encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "An example is the passenger class feature. This feature is already encoded in ordinal integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Pclass.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of an ordinal categorical is the Deck feature. We know the decks were named in order, with \"A\" at the top of the ship and \"G\" at the bottom. We could encode these into a range of [-4, 3] with the lowest number representing the lowest deck. We might choose this range as over 99.7% of data should fall within the [-3, 3] range of a standard normal variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_mapping = {\n",
    "    \"A\": 3,\n",
    "    \"B\": 2,\n",
    "    \"C\": 1,\n",
    "    \"D\": 0,\n",
    "    \"E\": -1,\n",
    "    \"F\": -2,\n",
    "    \"G\": -3,\n",
    "    \"No Cabin\": -4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mapping function to the Deck feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Nominal Categoricals\n",
    "\n",
    "Nominal categoricals have no order, so concepts such as mean, min, and max have no interpretation. One particularly popular method of encoding, known as One-Hot Encoding involves representing categorical variables as binary columns.\n",
    "\n",
    "In Pandas, we can use `get_dummies` to easily make one-hot features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply get_dummies to Embarked', Sex, and Title features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the dataset\n",
    "Finally, we'll drop the PassengerId feature and save the dataset as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"PassengerId\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/titanic_preprocessed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
